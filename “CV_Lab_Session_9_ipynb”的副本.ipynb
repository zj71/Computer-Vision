{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“CV Lab Session 9.ipynb”的副本",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zj71/Computer-Vision/blob/main/%E2%80%9CCV_Lab_Session_9_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdFS8P9_-RFt"
      },
      "source": [
        "## Lab Introduction\n",
        "This lab will introduce you to generative adversarial networks in [TensorFlow](https://www.tensorflow.org/). This is loosely based on the tutorials [here](https://www.tensorflow.org/tutorials/generative/dcgan). TensorFlow has [excellent documentation](https://www.tensorflow.org/api_docs/python/tf/keras). If you're running this in Colab, you can improve the speed of execution by selecting : Runtime | Change Runtime Type and selecting GPU (graphics processing unit) or [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13iIl_qX-n_g"
      },
      "source": [
        "#Imports\n",
        "We need to import tensorflow, commonly named as tf.\n",
        "We also use tf.keras and tf.keras.layers a lot as well, so we rename those."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRiuEyTQtezG"
      },
      "source": [
        "import tensorflow as tf\n",
        "keras = tf.keras\n",
        "layers = tf.keras.layers\n",
        "# Include numpy for basic stuff\n",
        "import numpy as np\n",
        "# matplotlib allows us to visualise our data.\n",
        "import matplotlib.pyplot as plt\n",
        "# Import a library for displaying models\n",
        "from IPython.display import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bCRKhHI-zyU"
      },
      "source": [
        "## Let's get some data\n",
        "For this lab we're going to initially look at the [MNIST handwritten digit dataset](http://yann.lecun.com/exdb/mnist/), as we did in the FML CNN lab. This dataset has been explored for a wide variety of models and serves as a standard benchmark for comparing/trying things. In the main part of this lab we do not need the image labels, just the data. The labels could be useful for the extension task of making a *conditional* GAN.\n",
        "\n",
        "1. Look at the size and shape of the data elements. How large are the images? \n",
        "2. Use [plt.imshow](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html?highlight=imshow#matplotlib.pyplot.imshow) to visualise some examples from the data. Check what the corresponding label for each example is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vlbu13obuC3t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "93f376ee-032b-43ba-c276-4d094f11da58"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "# Print the shapes of the array and the unique label \n",
        "print(x_train.shape, y_train.shape, np.unique(y_test))\n",
        "# Pick an example index\n",
        "example_idx = 0\n",
        "plt.imshow(x_train[example_idx,...],cmap='gray')\n",
        "# Print the label\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28) (60000,) [0 1 2 3 4 5 6 7 8 9]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_gn_-c4_GsX"
      },
      "source": [
        "# Prepare the data\n",
        "Again, we put the image data (which is normally from 0-255) into floating point numbers and scale so the values are between 0 and 1.\n",
        "We also don't need the test set for anything (why?), so we can append it to the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szIn3yPbuKZl"
      },
      "source": [
        "# Divide the image data to put it in the right range and convert to floating point numbers\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "# Preprocess the test data the same way\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "\n",
        "# Stick the data together\n",
        "x_data = np.concatenate([x_train, x_test], axis=0)\n",
        "\n",
        "# Reshape to (-1, im_height, im_width, 1)\n",
        "x_data = np.reshape(x_data, (-1, 28, 28,1))\n",
        "\n",
        "# We're not interested in the labels, but if we were...\n",
        "# Convert the labels to floating point\n",
        "y_train = y_train.astype(\"float32\")\n",
        "y_test = y_test.astype(\"float32\")\n",
        "y_data = np.concatenate([y_train, y_test], axis=0)\n",
        "\n",
        "# Define the BATCH_SIZE as a variable as we need it later\n",
        "BATCH_SIZE = 256\n",
        "# Prepare the training dataset into batches and shuffle the examples\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(x_data)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=10000).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fnl4leceur60"
      },
      "source": [
        "# Create the generator\n",
        "This function takes a vector of numbers of length z_size as input and produces an image! \n",
        "+ Trace through the function definition - try and fill in the missing code.\n",
        "+ Why do we initially map z into a 7x7 image? **hint** the original images have size 28x28.\n",
        "+ Are there any layers here that you've never seen before? What are they? what do you think they do?\n",
        "+ Instead of using [layers.UpSampling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D) and [layers.Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) you could use a [layers.Conv2DTranspose](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose). The approach taken here is generally considered to produce less artefacts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5vKivEwuLMG"
      },
      "source": [
        "def make_generator(z_size=32, layer_sizes=[256, 128, 64]):\n",
        "  \"\"\"\n",
        "  z_size specifies the latent variable dimension\n",
        "  layer_sizes: is a list containing the number of channels at each stage in the generator\n",
        "  \"\"\"\n",
        "  # Note that the Input size relates to the size of each instance\n",
        "  inputs = keras.Input(shape=(z_size,), name=\"z\")\n",
        "  net = inputs\n",
        "\n",
        "  # We first need to increase the size of our Tensor to reshape it into an image and\n",
        "  # apply some convolutions. Let's map our z into a 7x7xlayers[0].  \n",
        "  net = layers.Dense(7*7*layer_sizes[0],use_bias=False)(net)\n",
        "\n",
        "  # The Dense lay produces a new vector, which we can reshape to look like an image.\n",
        "  # Reshape to the right image size\n",
        "  net = layers.Reshape((?,?,layer_sizes[0]))(net)\n",
        "\n",
        "  # Apply batch normalisation followed by leaky relu (a slight variation on the relu activation function)\n",
        "  net = layers.BatchNormalization(scale=False, center=False)(net)\n",
        "  net = layers.LeakyReLU()(net)\n",
        "  # Let's do our first convolution\n",
        "  net = layers.Conv2D(layer_sizes[1], 5, padding='same', use_bias=False)(net)\n",
        "  net = layers.BatchNormalization(scale=False, center=False)(net)\n",
        "  net = layers.LeakyReLU()(net)\n",
        "\n",
        "  # We want to grow the size of the image, let's do this by applying bilinear upsampling\n",
        "  net = layers.UpSampling2D(size=(2,2))(net)\n",
        "\n",
        "  # Followed by convolution, batch norm and relu again\n",
        "  net = layers.Conv2D(layer_sizes[2], 5, padding='same', use_bias=False)(net)\n",
        "  net = layers.BatchNormalization(scale=False, center=False)(net)\n",
        "  net = layers.LeakyReLU()(net)\n",
        "\n",
        "    # We want to grow the size of the image, let's do this by applying bilinear upsampling\n",
        "  net = layers.UpSampling2D(size=(2,2))(net)\n",
        "\n",
        "  # We do one final convolution to produce the final image (use a sigmoid to put the values in the correct range)\n",
        "  net = layers.Conv2D(1, 5, padding='same', activation='sigmoid')(net)\n",
        "  \n",
        "\n",
        "  return keras.Model(inputs=inputs, outputs=net)\n",
        "\n",
        "# Test the function works - but don't save the result yet\n",
        "make_generator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3v9L1QxyOD7"
      },
      "source": [
        "# Create the discriminator\n",
        "This is just an image classifier, which will be used to identify if the images are from the training dataset or from the generator.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96IioHdQxvC0"
      },
      "source": [
        "def make_discriminator(layer_sizes=[64, 128]):\n",
        "  \"\"\"\n",
        "  layer_sizes: list containing the number of channels at each stage of the discriminator\n",
        "  \"\"\"\n",
        "  # Note that the Input size relates to the size of each instance\n",
        "  inputs = keras.Input(shape=(28,28,1), name=\"image\")\n",
        "  net = inputs\n",
        "\n",
        "  # We can also define the layer object, and store it in a Python variable and call it later\n",
        "  # We're more likely to want to do this for layers that contain model weights\n",
        "  conv1 = layers.Conv2D(filters=layer_sizes[0], kernel_size=5, padding='same', strides=(2,2))\n",
        "  net = conv1(net)\n",
        "  # Dropout might help a bit\n",
        "  #net = layers.Dropout(0.1)(net)\n",
        "  net = layers.LeakyReLU()(net)\n",
        "\n",
        "  conv2 = layers.Conv2D(filters=layer_sizes[1], kernel_size=5, padding='same', strides=(2,2))\n",
        "  net = conv2(net)\n",
        "  # Dropout might help a bit\n",
        "  #net = layers.Dropout(0.1)(net)\n",
        "  net = layers.LeakyReLU()(net)\n",
        "\n",
        "\n",
        "  # We need to flatten the spatial dimensions before putting it through the dense layers\n",
        "  net = layers.Flatten()(net)\n",
        "  dense1 = layers.Dense(units=1)\n",
        "  # Apply the dense layer\n",
        "  net = ?\n",
        "\n",
        "  return keras.Model(inputs=inputs, outputs=net)\n",
        "\n",
        "make_discriminator()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j0JuPd0oPxq"
      },
      "source": [
        "# Create our models\n",
        "We need to create our generator and discriminator models. \n",
        "+ Draw an image of what the untrained generator predicts for a random input vector.\n",
        "+ Examine the output of the untrained discriminator on that image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy61rg3XyTP6"
      },
      "source": [
        "noise_dim = 64\n",
        "\n",
        "generator = make_generator(z_size=noise_dim, layer_sizes=[256, 128, 64])\n",
        "\n",
        "noise = tf.random.normal([1, noise_dim])\n",
        "generated_image = generator(?, training=False)\n",
        "\n",
        "plt.imshow(?, cmap='gray')\n",
        "\n",
        "# Print the output of the discriminator\n",
        "discriminator = make_discriminator(layer_sizes=[64, 128])\n",
        "print(discriminator(?))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAo9WT6IohfX"
      },
      "source": [
        "# Define the model losses\n",
        "You don't need to write anything here - this is just where we define the losses for training the generator and the discriminator.\n",
        "+ Trace through the code, what is the difference between the discriminator and generator losses?\n",
        "+ An important thing to note is that we have different optimizers for our generator and discriminator. The learning rates are defined here, which are very important for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKd_kjZJ0vjQ"
      },
      "source": [
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True, label_smoothing=0.1)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "# Define the optimizers\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTVrjcxPs8di"
      },
      "source": [
        "# Define the training loop\n",
        "GANs are a bit different to train - we don't have a fixed loss function we're minimising as the models are competing against each other. The code below breaks down each iteration of training the generator and the discriminator.\n",
        "+ Trace through the code. What do you think a \"GradientTape\" does?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe_wjAjz04ei"
      },
      "source": [
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\". Making it faster to call thereafter\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "    # Create a pair of gradient tapes\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      # Generate some fake data from the noise input\n",
        "      generated_images = generator(noise, training=True)\n",
        "      # Call the discriminator on the real images\n",
        "      real_output = discriminator(images, training=True)\n",
        "      # Call the discriminator on the generated images\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "      # Calculate the losses\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "    # Calculate the gradient of the losses for the right variables\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    # Update the model variables\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    # Return the losses\n",
        "    return gen_loss, disc_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXl7DsdCBYy2"
      },
      "source": [
        "# Train the model\n",
        "Go through the dataset for a fixed number of epochs. Let's visualise the models progress as we go. After about 10 mins you should see the images start to appear like real images.\n",
        "+ GANs are notoriously tricky to train and there's a lot of variables that might affect our results. These include the optimizer learning rates, the generator/discriminator architectures, the batch size and the dimensionality of the noise variable z. Have an experiment with some of these factors\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6EjREvg098W"
      },
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    for idx, image_batch in enumerate(dataset):\n",
        "      losses = train_step(image_batch)\n",
        "      # Every N btaches, print the losses of that batch and draw some example images\n",
        "      if idx % 10 == 0:\n",
        "        print(losses)\n",
        "        noise = tf.random.normal([3, noise_dim])\n",
        "        generated_image = generator(noise, training=False)\n",
        "        plt.subplot(131)\n",
        "        plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n",
        "        plt.subplot(132)\n",
        "        plt.imshow(generated_image[1, :, :, 0], cmap='gray')\n",
        "        plt.subplot(133)\n",
        "        plt.imshow(generated_image[2, :, :, 0], cmap='gray')\n",
        "        plt.show()\n",
        "\n",
        "# \n",
        "train(train_dataset, 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZbYbZBQ70mF"
      },
      "source": [
        "# Extensions\n",
        "1. Once you've trained a model that works reasonably well. Generate an image for a random noise sample. Then generate another noise sample, and visualise how the images change as you change the latent values.\n",
        "1. Make a conditional GAN (i.e. where you can specify the label of the data). Look at this [example paper](https://arxiv.org/abs/1411.1784) for some ideas.\n",
        "2. Rather than regular convolutional layers. Try using residual convolutional blocks, which we talked about in the FML lectures. You can see an idea for implementing it (ResnetIdentityBlock) in the tutorial [here](https://www.tensorflow.org/tutorials/customization/custom_layers#implementing_custom_layers)\n",
        "3. Experiment with alternative datasets such as [FashionMNIST](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/fashion_mnist), which is similar to MNIST, but contains grayscale images of fashion items. Alternatively, look at a more complex (but still low-resolution) image dataset. [CIFAR 10](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10) or [CIFAR 100](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar100). Note that this dataset has images at (32x32,3) so you'll need to adjust the architecture (how?).\n",
        "4. We talked about CycleGAN in the lecture. Look at the [TensorFlow CycleGAN tutorial](https://www.tensorflow.org/tutorials/generative/cyclegan)\n",
        "5. Implement a residual block for the generator/discriminator, you could try sub-classing the [Layer base class](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer)\n",
        "6. Learn how to use [Tensorboard](https://www.tensorflow.org/tensorboard), an excellent tool for visualising your model training, which you can either run from [Colab](https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tensorboard_in_notebooks.ipynb) or install on your local machine\n",
        "7. Following the example in [this tutorial](https://www.tensorflow.org/tutorials/generative/dcgan) make a set of animations showing how the images change for a fixed set of random seeds over time."
      ]
    }
  ]
}